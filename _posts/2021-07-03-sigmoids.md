---
title: "Artificial Neurons: Sigmoids"
published: true
tag: nn-dl
thumbnail: /assets/weight-and-bias-affected-output.svg
---

This is the second post in a series on neural networks and deep learning. This
series is my attempt to get more familiar with the topic and is heavily based on
the [book by Michael Nielsen](http://neuralnetworksanddeeplearning.com/).

In this post we will be looking at something called a *sigmoid*. A sigmoids are
artificial neurons similar to [perceptrons]({% post_url
2021-06-27-perceptrons %})  discussed in the previous post. However, as will
become apparent in this post, they are extremely useful for neural networks and
machine learning.

<hr>

Suppose we have constructed a neural network using perceptrons. If we train this
network to do a certain task, we want to make sure that a small change in a bias
or weight, only has a small influence on the behaviour of the network. From what
we have seen in the [previous post]({% post_url 2021-06-27-perceptrons %}), this
is not the case with a network of perceptrons. A slight change of a bias or
weight can change a perceptron's output from 0 to 1 (or vice versa). In turn,
such a change can completely alter the behaviour of the neural network.

When training a neural network, it is not desirable to have neurons the respond in a binary fashion to their input. In other words, what we want is that for a small change in weight or bias, the output signal is only slightly changed. Perceptrons, with their basic binary (*on/off*-type) output, are simply not useable for this.


{:refdef: style="text-align: center;"}
![perceptron](/assets/weight-and-bias-affected-output.svg)
{: refdef}


**TO BE CONTINUED**

<hr>
